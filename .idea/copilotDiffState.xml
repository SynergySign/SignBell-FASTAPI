<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/processing/landmark_extractor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/processing/landmark_extractor.py" />
              <option name="originalContent" value="&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;Real-time Landmark Extraction (Inference) aligned with training coordinate_extractor.&#10;&#10;학습 시 사용된 coordinate_extractor 의 특징 생성 로직을 재현:&#10;- Pose 상체 관절 6개 (indices: 11,12,13,14,15,16)&#10;- 왼손: 손목 + 21 포인트 (총 22)&#10;- 오른손: 손목 + 21 포인트 (총 22)&#10;- 양 손목 3D 상대 거리 벡터 1개&#10;=&gt; 총 포인트: 6 + 22 + 22 + 1 = 51 포인트&#10;=&gt; 각 포인트 (x,y,z) 3차원 → frame feature dim = 153&#10;&#10;정규화 방식 (학습 코드와 동일):&#10;1. 안정 중심(stable center) = ( (좌/우 어깨 평균) + 코 ) / 2&#10;2. 각 포인트 상대좌표: (coord - stable_center) / normalization_scale  (scale=0.3)&#10;3. Pose Z 축: (z - stable_center_z)/scale * 0.7 (감쇠)&#10;4. 손목 좌표: 동일한 방식(감쇠 0.7)&#10;5. 손가락 좌표: 손목 기준 상대좌표 / scale, Z 는 0.6 감쇠&#10;6. 양손 손목 존재 시 오른손-왼손 3D 벡터 추가, 아니면 0&#10;7. 누락(없는 손/포즈) 시 0 채움 (학습 스크립트에서는 좌표 미검출 프레임을 제외했지만, 실시간 추론에서는 길이 안정화를 위해 0 벡터 padding 프레임을 허용할 수도 있음).&#10;&#10;실시간 추론 차이점:&#10;- 학습: 검출 실패 프레임은 skip.&#10;- 추론(옵션): skip 또는 zero-pad 중 선택 가능 (default: skip=False → zero pad 유지) ⇒ 모델이 학습 시 skip 을 했다면 skip=True 로 맞춰야 할 수도 있음.&#10;&#10;사용 예시:&#10;from processing.landmark_extractor import RealtimeLandmarkExtractor, extract_sequence_from_frames&#10;ext = RealtimeLandmarkExtractor(skip_missing=True)&#10;feat = ext.extract(frame_bytes)  # (153,) or None&#10;seq = extract_sequence_from_frames(frame_list, target_len=60, skip_missing=True)&#10;&#10;주의:&#10;- mediapipe / opencv / numpy 미설치 환경에서도 ImportError 로 서버 중단되지 않도록 방어.&#10;- 설치 필요 패키지: mediapipe, opencv-python, numpy&#10;&quot;&quot;&quot;&#10;from __future__ import annotations&#10;&#10;from dataclasses import dataclass, field&#10;from typing import List, Optional, Any&#10;&#10;# -------- Optional Dependencies (Graceful Fallback) --------&#10;try:&#10;    import numpy as np  # type: ignore&#10;except Exception:  # noqa: E722&#10;    np = None  # type: ignore&#10;&#10;try:&#10;    import mediapipe as mp  # type: ignore&#10;except Exception:  # noqa: E722&#10;    mp = None  # type: ignore&#10;&#10;try:&#10;    import cv2  # type: ignore&#10;except Exception:  # noqa: E722&#10;    cv2 = None  # type: ignore&#10;&#10;try:&#10;    from PIL import Image  # type: ignore&#10;    import io&#10;except Exception:  # noqa: E722&#10;    Image = None  # type: ignore&#10;    io = None  # type: ignore&#10;&#10;&#10;POSE_INDICES = [11, 12, 13, 14, 15, 16]&#10;NORMALIZATION_SCALE = 0.3&#10;POSE_Z_DAMPING = 0.7&#10;WRIST_Z_DAMPING = 0.7&#10;FINGER_Z_DAMPING = 0.6&#10;FRAME_FEATURE_DIM = 51 * 3  # 153&#10;&#10;&#10;def _decode_frame(frame_bytes: bytes):&#10;    &quot;&quot;&quot;Decode JPEG/PNG bytes -&gt; RGB ndarray (uint8). Returns None on failure.&quot;&quot;&quot;&#10;    if np is None:&#10;        return None&#10;    # 우선 OpenCV 사용 (속도 유리)&#10;    if cv2 is not None:&#10;        arr = np.frombuffer(frame_bytes, dtype=np.uint8)&#10;        img_bgr = cv2.imdecode(arr, cv2.IMREAD_COLOR)&#10;        if img_bgr is None:&#10;            return None&#10;        return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)&#10;    # Pillow fallback&#10;    if Image is not None and io is not None:&#10;        try:&#10;            with Image.open(io.BytesIO(frame_bytes)) as im:&#10;                return np.array(im.convert(&quot;RGB&quot;))&#10;        except Exception:  # noqa: E722&#10;            return None&#10;    return None&#10;&#10;&#10;@dataclass&#10;class RealtimeLandmarkExtractor:&#10;    static_image_mode: bool = False&#10;    model_complexity: int = 1&#10;    min_detection_confidence: float = 0.5&#10;    min_tracking_confidence: float = 0.5&#10;    skip_missing: bool = False  # True 면 pose 미검출 프레임은 None 반환 (학습과 유사), False 면 0 벡터&#10;    _holistic: Optional[Any] = field(init=False, default=None)&#10;    _ok: bool = field(init=False, default=False)&#10;&#10;    def __post_init__(self):&#10;        if mp is None or np is None:&#10;            self._ok = False&#10;            return&#10;        try:&#10;            self._holistic = mp.solutions.holistic.Holistic(&#10;                static_image_mode=self.static_image_mode,&#10;                model_complexity=self.model_complexity,&#10;                min_detection_confidence=self.min_detection_confidence,&#10;                min_tracking_confidence=self.min_tracking_confidence,&#10;                refine_face_landmarks=False,&#10;                enable_segmentation=False,&#10;            )&#10;            self._ok = True&#10;        except Exception:  # noqa: E722&#10;            self._ok = False&#10;            self._holistic = None&#10;&#10;    def available(self) -&gt; bool:&#10;        return self._ok and self._holistic is not None&#10;&#10;    def close(self):&#10;        if self._holistic is not None:&#10;            try:&#10;                self._holistic.close()  # type: ignore[attr-defined]&#10;            except Exception:  # noqa: E722&#10;                pass&#10;&#10;    # ------------- Core -------------&#10;    def extract(self, frame_bytes: bytes) -&gt; Optional[&quot;np.ndarray&quot;]:&#10;        &quot;&quot;&quot;단일 프레임 바이트 -&gt; (153,) float32 or None.&#10;        None 반환 조건:&#10;          - 의존성 없음&#10;          - 디코딩 실패&#10;          - pose 미검출 &amp; skip_missing=True&#10;        &quot;&quot;&quot;&#10;        if not self.available():&#10;            return None&#10;        rgb = _decode_frame(frame_bytes)&#10;        if rgb is None:&#10;            return None&#10;        try:&#10;            results = self._holistic.process(rgb)  # type: ignore[union-attr]&#10;        except Exception:  # noqa: E722&#10;            return None&#10;        if (not hasattr(results, &quot;pose_landmarks&quot;) or results.pose_landmarks is None):&#10;            if self.skip_missing:&#10;                return None&#10;            # zero frame 채움&#10;            if np is not None:&#10;                return np.zeros((FRAME_FEATURE_DIM,), dtype=np.float32)&#10;            return None&#10;&#10;        pose_lm = results.pose_landmarks.landmark&#10;        nose = pose_lm[0]&#10;        left_shoulder = pose_lm[11]&#10;        right_shoulder = pose_lm[12]&#10;        shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2&#10;        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2&#10;        shoulder_center_z = (left_shoulder.z + right_shoulder.z) / 2&#10;        stable_center_x = (shoulder_center_x + nose.x) / 2&#10;        stable_center_y = (shoulder_center_y + nose.y) / 2&#10;        stable_center_z = (shoulder_center_z + nose.z) / 2&#10;&#10;        feats: List[float] = []&#10;&#10;        def _rel(v, cx, cy, cz, scale, damp=1.0):&#10;            return (&#10;                (v.x - cx) / scale,&#10;                (v.y - cy) / scale,&#10;                ((v.z - cz) / scale) * damp,&#10;            )&#10;&#10;        # Pose 6 joints&#10;        for idx in POSE_INDICES:&#10;            v = pose_lm[idx]&#10;            rx, ry, rz = _rel(v, stable_center_x, stable_center_y, stable_center_z, NORMALIZATION_SCALE, POSE_Z_DAMPING)&#10;            feats.extend([rx, ry, rz])&#10;&#10;        # Left hand&#10;        left_wrist_pos = None&#10;        if hasattr(results, &quot;left_hand_landmarks&quot;) and results.left_hand_landmarks:&#10;            lms = results.left_hand_landmarks.landmark&#10;            wrist = lms[0]&#10;            wx, wy, wz = _rel(wrist, stable_center_x, stable_center_y, stable_center_z, NORMALIZATION_SCALE, WRIST_Z_DAMPING)&#10;            left_wrist_pos = (wx, wy, wz)&#10;            feats.extend([wx, wy, wz])  # wrist absolute relative to center&#10;            # fingers relative to wrist (scale = NORMALIZATION_SCALE)&#10;            for i in range(1, 21):&#10;                f = lms[i]&#10;                fx = (f.x - wrist.x) / NORMALIZATION_SCALE&#10;                fy = (f.y - wrist.y) / NORMALIZATION_SCALE&#10;                fz = ((f.z - wrist.z) / NORMALIZATION_SCALE) * FINGER_Z_DAMPING&#10;                feats.extend([fx, fy, fz])&#10;        else:&#10;            # 22 points * 3 values&#10;            feats.extend([0.0] * (22 * 3))&#10;&#10;        # Right hand&#10;        right_wrist_pos = None&#10;        if hasattr(results, &quot;right_hand_landmarks&quot;) and results.right_hand_landmarks:&#10;            lms = results.right_hand_landmarks.landmark&#10;            wrist = lms[0]&#10;            wx, wy, wz = _rel(wrist, stable_center_x, stable_center_y, stable_center_z, NORMALIZATION_SCALE, WRIST_Z_DAMPING)&#10;            right_wrist_pos = (wx, wy, wz)&#10;            feats.extend([wx, wy, wz])&#10;            for i in range(1, 21):&#10;                f = lms[i]&#10;                fx = (f.x - wrist.x) / NORMALIZATION_SCALE&#10;                fy = (f.y - wrist.y) / NORMALIZATION_SCALE&#10;                fz = ((f.z - wrist.z) / NORMALIZATION_SCALE) * FINGER_Z_DAMPING&#10;                feats.extend([fx, fy, fz])&#10;        else:&#10;            feats.extend([0.0] * (22 * 3))&#10;&#10;        # Hand distance vector&#10;        if left_wrist_pos and right_wrist_pos:&#10;            dx = right_wrist_pos[0] - left_wrist_pos[0]&#10;            dy = right_wrist_pos[1] - left_wrist_pos[1]&#10;            dz = right_wrist_pos[2] - left_wrist_pos[2]&#10;            feats.extend([dx, dy, dz])&#10;        else:&#10;            feats.extend([0.0, 0.0, 0.0])&#10;&#10;        if np is None:&#10;            return None&#10;        arr = np.asarray(feats, dtype=np.float32)&#10;        if arr.shape[0] != FRAME_FEATURE_DIM:&#10;            # 안전 실패: 차원 불일치 → 패딩/절단&#10;            if arr.shape[0] &lt; FRAME_FEATURE_DIM:&#10;                pad = np.zeros((FRAME_FEATURE_DIM - arr.shape[0],), dtype=np.float32)&#10;                arr = np.concatenate([arr, pad], axis=0)&#10;            else:&#10;                arr = arr[:FRAME_FEATURE_DIM]&#10;        return arr&#10;&#10;&#10;@dataclass&#10;class SequenceBuilder:&#10;    extractor: RealtimeLandmarkExtractor&#10;    frames: List[&quot;np.ndarray&quot;] = field(default_factory=list)&#10;&#10;    def add_frame(self, frame_bytes: bytes):&#10;        feat = self.extractor.extract(frame_bytes)&#10;        if feat is not None:&#10;            self.frames.append(feat)&#10;&#10;    def build(self, target_len: Optional[int] = None, pad_value: float = 0.0):&#10;        if np is None or not self.frames:&#10;            return None&#10;        seq = np.stack(self.frames, axis=0)  # (T, 153)&#10;        if target_len is not None:&#10;            T, F = seq.shape&#10;            if T &gt; target_len:&#10;                seq = seq[:target_len]&#10;            elif T &lt; target_len:&#10;                pad = np.full((target_len - T, F), pad_value, dtype=seq.dtype)&#10;                seq = np.concatenate([seq, pad], axis=0)&#10;        return seq  # (target_len, 153)&#10;&#10;&#10;def extract_sequence_from_frames(frames: List[bytes], target_len: Optional[int] = None, skip_missing: bool = False):&#10;    &quot;&quot;&quot;프레임 바이트 리스트 -&gt; (T|target_len, 153) numpy or None.&#10;    mediapipe / numpy 없으면 None.&#10;    &quot;&quot;&quot;&#10;    if np is None or mp is None:&#10;        return None&#10;    extractor = RealtimeLandmarkExtractor(skip_missing=skip_missing)&#10;    if not extractor.available():&#10;        return None&#10;    builder = SequenceBuilder(extractor)&#10;    for fb in frames:&#10;        builder.add_frame(fb)&#10;    seq = builder.build(target_len=target_len)&#10;    extractor.close()&#10;    return seq&#10;&#10;&#10;__all__ = [&#10;    &quot;RealtimeLandmarkExtractor&quot;,&#10;    &quot;SequenceBuilder&quot;,&#10;    &quot;extract_sequence_from_frames&quot;,&#10;    &quot;FRAME_FEATURE_DIM&quot;,&#10;]" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>