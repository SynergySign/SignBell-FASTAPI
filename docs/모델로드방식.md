## 1\. 학습된 모델 안정적 로드 (오류 방지 전략)

이 문서는 학습된 모델을 서버에 안정적으로 배치하고 로드하기 위한 권장 방식과 흔히 발생하는 오류 및 대응 방법을 정리합니다. 현재 레포에서는 `processing/predictor.py`의 `get_predictor()`가 기본적으로 `models/cnn_bilstm_attention_model.pth`를 찾도록 되어 있으므로, 문서의 예시도 이를 염두에 두고 설명합니다.

### 1.1. 모델 변환·저장 단계 (Optimizing for Inference)

주요 문제와 권장 대응

- 변환 후 성능 저하/오류
  - 조치: 변환(ONNX, TorchScript 등) 후 반드시 유효성 검증을 수행합니다. 더미 입력(Dummy Input)을 주어 변환 전 모델과 출력(또는 logits/probabilities)이 수치적으로 일치하는지 확인하세요.
  - PyTorch는 저장 전에 `model.eval()`을 호출하여 학습 모드 관련 동작(드롭아웃, 배치노멀 등)이 추론 모드로 고정되었는지 확인합니다.

- 입력 형태(Shape) 불일치
  - 조치: WebRTC/DataChannel로 들어오는 시퀀스(프레임 길이, feature dim)가 모델이 기대하는 입력 텐서 형태와 일치하는지 문서화하고 검증합니다. 배치 차원과 시퀀스 길이에 대한 정책(고정/동적)을 명확히 하세요.

-----

### 1.2. 모델 파일 배치 단계 (Robust Path Handling)

- FileNotFoundError (모델 파일 미발견)
  - 조치: `pathlib`으로 절대 경로를 구성하고, 서버 실행 파일 기준(예: `main.py`의 위치)으로 모델 경로를 정합니다. `get_predictor()`에서 모델이 없으면 FileNotFoundError를 발생시키므로 배포 전에 파일 존재 여부를 확인하세요.

- 의존성(클래스) 로드 실패
  - 조치: checkpoint가 Python 객체 직렬화(pickle)를 포함하는 경우, 모델 구조를 정의한 클래스가 import 가능한 상태여야 합니다. 또는 가능한 경우 `state_dict`만 저장하여 클래스 의존성을 줄이세요.

💡 경로 처리 예시 (Pathlib, 메인 파일 기준)

```text
from pathlib import Path

# main.py 기준 프로젝트 루트
BASE_DIR = Path(__file__).resolve().parent
MODEL_PATH = BASE_DIR / "models" / "cnn_bilstm_attention_model.pth"
```

- 참고: 레포의 `processing/get_predictor()`는 위와 유사한 방식으로 `models/cnn_bilstm_attention_model.pth`를 찾습니다.

-----

### 1.3. 서버 시작 시 모델 로드 (FastAPI startup)

- 권장 패턴
  - 모델 로드는 `@app.on_event("startup")`에서 수행하거나, `get_predictor()`처럼 동기적으로 초기화된 객체를 앱 상태(`app.state`)에 넣어 다른 핸들러에서 사용하도록 합니다.
  - 로드 실패 시 즉시 프로세스를 죽이기보다는 예외를 로깅하고, 헬스 체크(`/health`)에서 문제를 알리거나 더미 대체 로직으로 서비스 연속성을 확보할 수 있습니다.

- 예외 처리
  - FileNotFoundError: 명확한 에러 메시지 출력 + 배포 문서(모델 준비 단계) 안내
  - 클래스 매핑 오류: torch.load가 pickle 기반으로 복원할 때 참조되는 클래스가 없으면 실패합니다. 이를 해결하려면 모델을 저장할 때 `state_dict`로 저장하거나, 서버 시작 전에 필요한 클래스(모델 정의)를 import/매핑해 주세요.

다음은 `startup`에서의 안정적 처리 예시(요약)

```text
@app.on_event("startup")
async def startup_event():
    try:
        app.state.predictor = get_predictor()
        print("Predictor loaded")
    except FileNotFoundError as e:
        print(f"Model file not found: {e}")
        app.state.predictor = None
    except Exception as e:
        print(f"Unexpected error while loading predictor: {e}")
        app.state.predictor = None
```

- 참고: 본 레포의 `main.py`는 `get_predictor()`를 호출하고 `app.state.ss.predictor`에 보관합니다. 또한, torch.load 관련 클래스 참조 문제를 완화하기 위해 `sys.modules['__main__']`에 필요한 클래스명을 매핑하는 코드를 사용하고 있습니다.

-----

### 1.4. Torch Checkpoint 관련 주의사항

- 가능하면 `state_dict` 형태로 저장하세요. (pickle 기반 전체 객체 저장보다 안전)
- `label_encoder` 등 추가 객체가 checkpoint에 포함되어 있다면, 로드 시 존재 여부를 확인하고 로드 실패 시 대체 동작을 정의하세요.
- CPU/GPU 매핑: `torch.load(model_path, map_location=device)`를 명시하여 환경에 따라 로드 위치를 제어하세요.

-----

### 1.5. ONNX / TorchScript 고려사항

- ONNX: 런타임(ONNX Runtime)에서 session을 만들고, 입력 크기(dynamic/static)를 명확히 지정합니다. 변환 후 동일성 검증(숫자 비교)을 필수로 하세요.
- TorchScript: `torch.jit.trace/script`로 변환하면 pickle 의존성이 줄어듭니다.

-----

### 1.6. 권장 체크리스트 (배포 전)

- [ ] 모델 파일(`models/cnn_bilstm_attention_model.pth`)이 배포 패키지에 포함되어 있는지 확인
- [ ] `get_predictor()` 로컬에서 수동 호출하여 모델 로드 테스트 완료
- [ ] `startup` 이벤트에서 실패 시 `app.state`가 None이 되도록 처리하고 `/health`가 실패를 알려줌
- [ ] checkpoint 저장 시 `state_dict`를 우선 고려
- [ ] 모델 로드 시 소요 시간(워밍업 포함)을 로그에 남기고 필요 시 타임아웃/문서화

-----

문서의 목적은 운영 환경에서 모델 로드 시 발생하는 흔한 문제를 미리 점검하고, 레포의 현재 구조(예: `get_predictor()`의 경로, `main.py`에서의 app.state 사용)를 반영한 권장 방식을 제공하는 것입니다.

