## 1\. 학습된 모델 안정적 로드 (오류 방지 전략)

### 1.1. 모델 최적화 및 저장 단계 (Optimizing for Inference)

| 예상되는 주요 오류 | 안정적인 구현 방법 |
| :--- | :--- |
| **변환 후 성능 저하/오류** | 학습 모델과 추론 모델의 **유효성 검증(Validation)** 누락. | 1. **모델 유효성 검증:** 변환된 `.onnx` 또는 `TorchScript` 모델에 \*\*실제 입력 데이터(Dummy Input)\*\*를 넣어보고, **변환 전 원본 모델의 출력 값과 100% 동일한지** **수치적**으로 확인합니다. / 2. **추론 전용 모드:** PyTorch의 경우 `model.eval()`을 호출하여 드롭아웃(Dropout) 등 학습 관련 레이어를 비활성화한 후 저장해야 합니다. |
| **입력 형태(Shape) 불일치** | WebRTC에서 수신되는 **프레임 시퀀스의 형태**와 모델이 기대하는 **입력 텐서 형태**가 다름. | \*\*입력 텐서의 크기(Shape)\*\*를 명확하게 정의합니다. 특히 \*\*배치 크기(Batch Size)\*\*와 \*\*시퀀스 길이(9\~10초 프레임 수)\*\*를 **고정(Static)** 또는 \*\*동적(Dynamic)\*\*으로 지정하여 저장 시 오류를 방지해야 합니다. |

-----

### 1.2. 모델 파일 배치 단계 (Robust Path Handling)

| 예상되는 주요 오류 | 안정적인 구현 방법 |
| :--- | :--- |
| **`FileNotFoundError`** | 서버 코드가 실행되는 위치와 `models` 디렉토리 간의 **상대 경로 오류** 발생. | 1. **`pathlib` 사용:** Python의 `pathlib` 라이브러리를 사용하여 운영체제에 독립적인 **절대 경로**를 생성합니다. / 2. **루트 디렉토리 정의:** FAST API의 메인 실행 파일(`main.py` 등)이 위치한 디렉토리를 기준으로 `models` 디렉토리의 **절대 경로**를 정의하고 사용합니다. |
| **의존성(Classes) 로드 실패** | 모델 구조 정의에 필요한 **클래스(Class)** 파일 로드 실패. | **추론에 필요한 모든 모듈 및 클래스** (예: 사용자 정의 데이터 로더, 모델 구조 파일)를 서버 시작 전에 **미리 임포트**하고, 특히 `.onnx`나 `TorchScript`처럼 **직렬화된 모델**이 해당 클래스에 의존하지 않도록 **독립적인 로드** 방법을 사용해야 합니다. |

#### **💡 경로 처리 예시 (Pathlib)**

```python
from pathlib import Path

# 현재 파일(서버 실행 파일)의 부모 디렉토리 (프로젝트 루트)
BASE_DIR = Path(__file__).resolve().parent

# 모델 디렉토리의 절대 경로
MODEL_DIR = BASE_DIR / "models"

# 실제 모델 파일의 절대 경로
MODEL_PATH = MODEL_DIR / "optimized_sign_language_model.onnx"

# 1.3에서 이 절대 경로(MODEL_PATH)를 사용합니다.
```

-----

### 1.3. 서버 시작 시 모델 로드 단계 (Global & Asynchronous Initialization)

| 예상되는 주요 오류 | 안정적인 구현 방법 |
| :--- | :--- |
| **모델 로드 실패 시 서버 다운** | 파일 손상, 경로 오류, 메모리 부족 등으로 로드 실패 시 서버가 바로 종료됨. | 1. **견고한 예외 처리:** `try...except` 구문을 사용하여 모델 로딩 코드를 감쌉니다. 로드에 실패하더라도 서버가 즉시 다운되지 않도록 **에러 로깅** 후 graceful하게 종료하거나 대체 로직을 실행합니다. / 2. **메모리 확인:** 고성능 모델의 경우, 로드 전 서버의 **사용 가능한 메모리**를 확인하고, 로드 시 **메모리 최적화 옵션**을 활성화합니다. |
| **모델이 전역으로 접근 불가능** | `startup` 이벤트 내에서 정의된 모델 객체가 다른 엔드포인트에서 `None`이 됨. | **`app.state` 사용:** FAST API의 애플리케이션 상태 객체에 로드된 모델을 저장하여 모든 요청 처리 함수에서 **안정적으로 접근 가능**하도록 합니다. |
| **로드 지연으로 인한 타임아웃** | 모델 로딩 시간이 길어 Uvicorn 서버 시작 시간이 초과됨. | **비동기 로드 확인:** `uvicorn`의 **`startup` 이벤트**는 동기적으로 실행되지만, 로드하는 라이브러리(e.g., ONNX Runtime) 자체의 초기화가 길 수 있습니다. **로그를 통해 로딩 시간을 체크**하고, 장시간 로딩 시 `uvicorn`의 **타임아웃 설정**을 조정할 수 있습니다. |

#### **💡 `@app.on_event("startup")` 안정화 코드 예시**

```python
from fastapi import FastAPI
# 1.2에서 정의한 MODEL_PATH 임포트
# from .paths import MODEL_PATH 

# 추론 라이브러리 임포트 (예: ONNX Runtime)
# import onnxruntime as ort

app = FastAPI()

@app.on_event("startup")
async def load_model_and_set_state():
    """
    서버 시작 시 모델을 로드하고 앱 상태에 저장하여 전역 접근 가능하게 합니다.
    """
    try:
        # 1. 모델 로드 로직
        # session = ort.InferenceSession(str(MODEL_PATH))
        
        # 2. MediaPipe 파이프라인 초기화 로직
        # mediapipe_pipeline = initialize_mediapipe_pipeline()
        
        # 3. FAST API 상태(state)에 저장 (가장 중요)
        app.state.ml_session = session 
        app.state.mp_pipeline = mediapipe_pipeline
        
        print("✅ 모델 및 MediaPipe 파이프라인 로드 완료. (app.state 저장)")
        
    except FileNotFoundError:
        print(f"❌ 오류: 모델 파일 경로를 찾을 수 없습니다: {MODEL_PATH}")
        # 서버를 시작하지 않고 종료하거나, 안전한 대체 로직 실행
        raise
    except Exception as e:
        print(f"❌ 오류: 모델 로드 중 예상치 못한 에러 발생: {e}")
        # 상세 에러 로깅
        raise

# 실제 엔드포인트에서 모델 사용 예시
# @app.post("/predict")
# async def predict_sign_language(data: dict):
#     # app.state를 통해 로드된 모델에 접근
#     session = app.state.ml_session
#     # ... 추론 로직 실행 ...
```