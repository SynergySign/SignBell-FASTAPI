import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
import math

# --- ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ---

class PositionalEncoding(nn.Module):
    """
    Transformer ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” Positional Encodingì„ êµ¬í˜„í•©ë‹ˆë‹¤.
    ëª¨ë¸ì´ ì €ì¥ë  ë•Œ ì´ í´ë˜ìŠ¤ë„ í•¨ê»˜ ì €ì¥ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
    """
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: (batch_size, sequence_length, d_model)
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

# ì°¸ê³ : ì´ í´ë˜ìŠ¤ëŠ” 'models/cnn_bilstm_attention_model.pth' íŒŒì¼ì´ ì €ì¥ë  ë•Œ ì‚¬ìš©ëœ
# ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
# ë§Œì•½ ì‹¤ì œ ëª¨ë¸ê³¼ êµ¬ì¡°ê°€ ë‹¤ë¥¼ ê²½ìš°, ì—¬ê¸°ì„œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
class CNN_BiLSTM_Attention(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes, sequence_length):
        super(CNN_BiLSTM_Attention, self).__init__()
        # ì´ ë¶€ë¶„ì€ ëª¨ë¸ì˜ ì‹¤ì œ êµ¬ì¡°ì— ë”°ë¼ ë³€ê²½ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        # ì•„ë˜ëŠ” íŒŒì¼ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê¸°ë³¸ì ì¸ ì¶”ì •ì…ë‹ˆë‹¤.
        self.feature_embedding = nn.Linear(input_size, hidden_size)
        self.pos_encoder = PositionalEncoding(hidden_size)
        self.cnn = nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=True)
        self.attention = nn.Linear(hidden_size * 2, 1)
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
        # x: (batch_size, sequence_length, input_size)

        # 1. Feature Embedding & Positional Encoding
        x = self.feature_embedding(x) # (batch_size, sequence_length, hidden_size)
        x = self.pos_encoder(x)       # (batch_size, sequence_length, hidden_size)

        # 2. CNN
        x = x.permute(0, 2, 1) # (batch_size, hidden_size, sequence_length)
        x = self.cnn(x)
        x = self.relu(x)
        x = x.permute(0, 2, 1) # (batch_size, sequence_length, hidden_size)
        
        # 3. BiLSTM
        lstm_out, _ = self.lstm(x) # (batch_size, sequence_length, hidden_size * 2)
        
        # 4. Attention
        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)
        context_vector = torch.sum(attention_weights * lstm_out, dim=1) # (batch_size, hidden_size * 2)
        
        out = self.fc(context_vector)
        return out

# --- Predictor ëª¨ë“ˆ ---
class Predictor:
    def __init__(self, model_path, num_classes=10, sequence_length=30, input_size=147):
        """
        ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì¶”ë¡ ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
        :param model_path: .pth ëª¨ë¸ íŒŒì¼ì˜ ê²½ë¡œ
        :param num_classes: ëª¨ë¸ì˜ ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜
        :param sequence_length: ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
        :param input_size: ê° í”„ë ˆì„ì˜ ëœë“œë§ˆí¬ íŠ¹ì„± ìˆ˜ (e.g., 49 * 3 = 147)
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ¤– Predictorê°€ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {self.device}")

        # ëª¨ë¸ êµ¬ì¡° ì •ì˜
        # ì°¸ê³ : hidden_size, num_layersëŠ” ì €ì¥ëœ ëª¨ë¸ê³¼ ë™ì¼í•œ ê°’ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        self.model = CNN_BiLSTM_Attention(
            input_size=input_size,
            hidden_size=256,
            num_layers=2,
            num_classes=num_classes,
            sequence_length=sequence_length
        ).to(self.device)

        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
        
        # TODO: í´ë˜ìŠ¤ ë ˆì´ë¸” ë§µì„ ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
        self.class_labels = {i: f'Sign_{i}' for i in range(num_classes)}
        print(f"âœ… Predictor ì´ˆê¸°í™” ì™„ë£Œ. ëª¨ë¸: {model_path}")

    def predict(self, landmark_sequence: list) -> str:
        """
        ëœë“œë§ˆí¬ ì‹œí€€ìŠ¤ë¥¼ ë°›ì•„ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        :param landmark_sequence: ëœë“œë§ˆí¬ ë°ì´í„°ì˜ ë¦¬ìŠ¤íŠ¸ (ê¸¸ì´ëŠ” sequence_lengthì™€ ìœ ì‚¬í•´ì•¼ í•¨)
        :return: ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ë ˆì´ë¸” (ë¬¸ìì—´)
        """
        if not landmark_sequence:
            return "ëœë“œë§ˆí¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        landmarks_np = np.array(landmark_sequence, dtype=np.float32)

        # ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬ (ì •ê·œí™”, í¬ê¸° ì¡°ì ˆ ë“±)
        # TODO: í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì—¬ê¸°ì— ë™ì¼í•˜ê²Œ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
        # ì˜ˆ: landmarks_np = (landmarks_np - mean) / std
        
        # ëª¨ë¸ ì…ë ¥ í˜•íƒœì— ë§ê²Œ í…ì„œë¡œ ë³€í™˜ (batch_size=1)
        input_tensor = torch.from_numpy(landmarks_np).unsqueeze(0).to(self.device)

        with torch.no_grad():
            outputs = self.model(input_tensor)
            _, predicted_idx = torch.max(outputs, 1)
            
        predicted_label = self.class_labels.get(predicted_idx.item(), "ì•Œ ìˆ˜ ì—†ëŠ” ê¸°í˜¸")
        
        return predicted_label

def get_predictor():
    """Predictor ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜"""
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).resolve().parent.parent
    model_path = base_dir / "models" / "cnn_bilstm_attention_model.pth"

    if not model_path.exists():
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    # TODO: num_classes, sequence_length, input_sizeë¥¼ ì‹¤ì œ ëª¨ë¸ì— ë§ê²Œ ì¡°ì •
    predictor = Predictor(
        model_path=str(model_path),
        num_classes=10,      # ì˜ˆì‹œ: 10ê°œì˜ ìˆ˜ì–´ ë‹¨ì–´
        sequence_length=30,  # ì˜ˆì‹œ: 30í”„ë ˆì„
        input_size=147       # ëœë“œë§ˆí¬ ì¶”ì¶œê¸°(147D)ì— ë§ê²Œ ìˆ˜ì •
    )
    return predictor

if __name__ == '__main__':
    # ëª¨ë“ˆ ë‹¨ë… í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
    try:
        print("Predictor ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        predictor = get_predictor()
        
        # ë”ë¯¸ ë°ì´í„° ìƒì„± (batch=1, sequence_length=30, input_size=147)
        dummy_input = np.random.rand(30, 147).tolist()

        prediction = predictor.predict(dummy_input)
        print(f"ğŸš€ ë”ë¯¸ ë°ì´í„° ì˜ˆì¸¡ ê²°ê³¼: {prediction}")
        
        print("Predictor ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì„±ê³µ.")
    except Exception as e:
        print(f"âŒ Predictor ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
