import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
import math

# --- ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ---
# í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸(cnn_bilstm_attention_classifier.py)ì™€ ë™ì¼í•œ ëª¨ë¸ êµ¬ì¡°ë¡œ êµì²´í•©ë‹ˆë‹¤.

class PositionalEncoding(nn.Module):
    """Sinusoidal Positional Encoding (batch_first)"""
    def __init__(self, d_model, max_len=500):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]


class CNN_BiLSTM_Attention(nn.Module):
    """í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜"""

    def __init__(self, input_size=147, num_classes=7,
                 cnn_channels=[64, 128, 256],
                 lstm_hidden=128,
                 dropout=0.5):
        super().__init__()

        # 1D-CNN Layers
        self.conv_layers = nn.ModuleList()
        in_channels = input_size

        for out_channels in cnn_channels:
            self.conv_layers.append(nn.Sequential(
                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm1d(out_channels),
                nn.ReLU(),
                nn.Dropout(dropout * 0.5)
            ))
            in_channels = out_channels

        # Bi-LSTM
        self.lstm = nn.LSTM(
            input_size=cnn_channels[-1],
            hidden_size=lstm_hidden,
            num_layers=2,
            batch_first=True,
            bidirectional=True,
            dropout=dropout
        )

        # Multi-Head Attention
        self.attention = nn.MultiheadAttention(
            embed_dim=lstm_hidden * 2,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )

        # Classification Head
        self.classifier = nn.Sequential(
            nn.Linear(lstm_hidden * 2, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_classes)
        )

        # Layer Normalization
        self.layer_norm = nn.LayerNorm(lstm_hidden * 2)

        # ë™ì‘ ë³€í™”ì  ê°€ì¤‘ì¹˜ í•™ìŠµ
        self.temporal_weight = nn.Parameter(torch.tensor(0.1))
        self.change_detector = nn.Sequential(
            nn.Linear(lstm_hidden * 2, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

        # Positional Encoding
        self.pos_encoding = PositionalEncoding(d_model=lstm_hidden * 2, max_len=500)

    def forward(self, x):
        batch_size, seq_len, features = x.size()

        x_cnn = x.transpose(1, 2)

        for conv_layer in self.conv_layers:
            x_cnn = conv_layer(x_cnn)

        x_cnn = x_cnn.transpose(1, 2)

        lstm_out, _ = self.lstm(x_cnn)
        lstm_out = self.layer_norm(lstm_out)
        lstm_out = self.pos_encoding(lstm_out)

        change_scores = self.change_detector(lstm_out).squeeze(-1)
        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)

        change_weights = change_scores.unsqueeze(-1) * self.temporal_weight
        weighted_attn = attn_out * (1 + change_weights)

        context_vector = weighted_attn.mean(dim=1)
        output = self.classifier(context_vector)

        # ì¶”ë¡  ì‹œì—ëŠ” outputë§Œ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
        return output

# --- Predictor ëª¨ë“ˆ ---
class Predictor:
    def __init__(self, model_path, sequence_length=300, input_size=147):
        """
        ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì¶”ë¡ ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ¤– Predictorê°€ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {self.device}")

        # 1. ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¨¼ì € ë¡œë“œí•˜ì—¬ ëª¨ë¸ì˜ ì„¤ì •ì„ í™•ì¸í•©ë‹ˆë‹¤.
        checkpoint = torch.load(model_path, map_location=self.device)

        # 2. ì²´í¬í¬ì¸íŠ¸ì—ì„œ í´ë˜ìŠ¤ ê°œìˆ˜ì™€ ë ˆì´ë¸” ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if 'label_encoder' in checkpoint:
            self.label_encoder = checkpoint['label_encoder']
            num_classes = len(self.label_encoder.classes_)
            self.class_labels = {i: label for i, label in enumerate(self.label_encoder.classes_)}
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ {num_classes}ê°œì˜ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        else:
            # label_encoderê°€ ì—†ëŠ” ê²½ìš°, state_dictì—ì„œ ì§ì ‘ ì¶”ë¡ í•©ë‹ˆë‹¤.
            try:
                num_classes = checkpoint['model_state_dict']['classifier.6.bias'].shape[0]
                self.class_labels = {i: f'Sign_{i}' for i in range(num_classes)}
                print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ì— ë ˆì´ë¸” ì •ë³´ê°€ ì—†ì–´, ëª¨ë¸ ê°€ì¤‘ì¹˜ì—ì„œ {num_classes}ê°œ í´ë˜ìŠ¤ë¥¼ ì¶”ë¡ í–ˆìŠµë‹ˆë‹¤.")
            except KeyError:
                raise ValueError("ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í´ë˜ìŠ¤ ê°œìˆ˜ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 3. í™•ì¸ëœ í´ë˜ìŠ¤ ê°œìˆ˜ë¡œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
        self.model = CNN_BiLSTM_Attention(
            input_size=input_size,
            num_classes=num_classes, # ë™ì ìœ¼ë¡œ ì„¤ì •ëœ í´ë˜ìŠ¤ ê°œìˆ˜ ì‚¬ìš©
            cnn_channels=[64, 128, 256],
            lstm_hidden=128,
            dropout=0.5
        ).to(self.device)

        # 4. ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        elif 'model' in checkpoint and isinstance(checkpoint['model'], nn.Module):
            state_dict = checkpoint['model'].state_dict()
        else:
            state_dict = checkpoint

        self.model.load_state_dict(state_dict)
        self.model.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •

        print(f"âœ… Predictor ì´ˆê¸°í™” ì™„ë£Œ. ëª¨ë¸: {model_path}")

    def predict(self, landmark_sequence: np.ndarray) -> tuple[str, float]:
        """
        ëœë“œë§ˆí¬ ì‹œí€€ìŠ¤ë¥¼ ë°›ì•„ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , ë ˆì´ë¸”ê³¼ ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        :param landmark_sequence: ëœë“œë§ˆí¬ ë°ì´í„°ì˜ numpy ë°°ì—´
        :return: (ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ë ˆì´ë¸”, ì‹ ë¢°ë„ ì ìˆ˜) íŠœí”Œ
        """
        # NumPy ë°°ì—´ì˜ ìœ íš¨ì„±ì„ ì˜¬ë°”ë¥´ê²Œ í™•ì¸í•©ë‹ˆë‹¤.
        if landmark_sequence is None or landmark_sequence.size == 0:
            return "ëœë“œë§ˆí¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", 0.0

        # landmark_sequenceê°€ ì´ë¯¸ numpy ë°°ì—´ì´ë¼ê³  ê°€ì •í•˜ê³  íƒ€ì… ë³€í™˜
        landmarks_np = landmark_sequence.astype(np.float32)

        # ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬ (ì •ê·œí™”, í¬ê¸° ì¡°ì ˆ ë“±)
        # TODO: í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì—¬ê¸°ì— ë™ì¼í•˜ê²Œ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
        # ì˜ˆ: landmarks_np = (landmarks_np - mean) / std
        
        # ì…ë ¥ ë°ì´í„° í¬ê¸° ì¡°ì ˆ (íŒ¨ë”©/ìë¥´ê¸°)
        num_frames = landmarks_np.shape[0]
        max_frames = 300 # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ max_framesì™€ ë™ì¼í•´ì•¼ í•¨
        if num_frames < max_frames:
            pad_size = max_frames - num_frames
            # íŒ¨ë”© ë°°ì—´ì˜ ë°ì´í„° íƒ€ì…ë„ float32ë¡œ ëª…ì‹œí•˜ì—¬ íƒ€ì… ë¶ˆì¼ì¹˜ ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
            landmarks_np = np.vstack([
                landmarks_np,
                np.zeros((pad_size, landmarks_np.shape[1]), dtype=np.float32)
            ])
        elif num_frames > max_frames:
            landmarks_np = landmarks_np[:max_frames]

        # ëª¨ë¸ ì…ë ¥ í˜•íƒœì— ë§ê²Œ í…ì„œë¡œ ë³€í™˜ (batch_size=1)
        input_tensor = torch.from_numpy(landmarks_np).unsqueeze(0).to(self.device)

        with torch.no_grad():
            outputs = self.model(input_tensor)
            # Softmaxë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  ê³„ì‚°
            probabilities = torch.softmax(outputs, dim=1)
            # ê°€ì¥ ë†’ì€ í™•ë¥ (ì ìˆ˜)ê³¼ í•´ë‹¹ ì¸ë±ìŠ¤(ì˜ˆì¸¡)ë¥¼ ê°€ì ¸ì˜´
            score, predicted_idx = torch.max(probabilities, 1)

        predicted_label = self.class_labels.get(predicted_idx.item(), "ì•Œ ìˆ˜ ì—†ëŠ” ê¸°í˜¸")
        
        return predicted_label, score.item()

def get_predictor():
    """Predictor ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜"""
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).resolve().parent.parent
    model_path = base_dir / "models" / "cnn_bilstm_attention_model.pth"

    if not model_path.exists():
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    # Predictor ìƒì„± ì‹œ num_classes ì¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    # í´ë˜ìŠ¤ ê°œìˆ˜ëŠ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì§ì ‘ ì½ì–´ì˜¤ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
    predictor = Predictor(
        model_path=str(model_path),
        sequence_length=300,
        input_size=147
    )
    return predictor

if __name__ == '__main__':
    # ëª¨ë“ˆ ë‹¨ë… í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
    try:
        print("Predictor ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        predictor = get_predictor()
        
        # ë”ë¯¸ ë°ì´í„° ìƒì„± (batch=1, sequence_length=30, input_size=147)
        dummy_input = np.random.rand(30, 147)

        prediction, score = predictor.predict(dummy_input)
        print(f"ğŸš€ ë”ë¯¸ ë°ì´í„° ì˜ˆì¸¡ ê²°ê³¼: {prediction} (Score: {score:.4f})")

        print("Predictor ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì„±ê³µ.")
    except Exception as e:
        print(f"âŒ Predictor ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
