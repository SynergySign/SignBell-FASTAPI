"""
ì‹¤ì‹œê°„ ìˆ˜í™” ì˜ˆì¸¡ì„ ìœ„í•œ ëª¨ë¸ê³¼ ì¢Œí‘œ ì¶”ì¶œê¸°
"""

import torch
import torch.nn as nn
import numpy as np
import cv2
import mediapipe as mp
from sklearn.preprocessing import LabelEncoder
from pathlib import Path
from typing import Optional, List
from cnn_bilstm_attention_classifier import CNN_BiLSTM_Attention



class CoordinateExtractor:
    """ì¢Œí‘œ ì¶”ì¶œê¸° (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë¡œì§)"""

    def __init__(self):
        self.mp_holistic = mp.solutions.holistic
        self.holistic = self.mp_holistic.Holistic(
            static_image_mode=False,
            model_complexity=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        # í•™ìŠµ ì‹œì™€ ë™ì¼í•œ Pose ì¸ë±ìŠ¤
        self.pose_indices = [11, 12, 13, 14, 15, 16]  # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©

    def extract_from_frame(self, frame: np.ndarray) -> Optional[List[List[float]]]:
        """ë‹¨ì¼ í”„ë ˆì„ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ"""
        coords = self._process_frame(frame)
        return coords if coords else None

    def _process_frame(self, frame: np.ndarray) -> List[List[float]]:
        """í”„ë ˆì„ ì²˜ë¦¬ (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë¡œì§)"""
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image_rgb.flags.writeable = False
        results = self.holistic.process(image_rgb)

        if not hasattr(results, 'pose_landmarks') or results.pose_landmarks is None:
            return []

        pose_landmarks = results.pose_landmarks.landmark

        # ê¸°ì¤€ì : ì–´ê¹¨ ì¤‘ì‹¬ + ì½” í‰ê·  (í•™ìŠµ ì‹œì™€ ë™ì¼)
        left_shoulder = pose_landmarks[11]
        right_shoulder = pose_landmarks[12]
        nose = pose_landmarks[0]
        shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
        shoulder_center_z = (left_shoulder.z + right_shoulder.z) / 2
        stable_center_x = (shoulder_center_x + nose.x) / 2
        stable_center_y = (shoulder_center_y + nose.y) / 2
        stable_center_z = (shoulder_center_z + nose.z) / 2

        normalization_scale = 0.3
        keypoints = []

        # Pose 3D ìƒëŒ€ì¢Œí‘œ (Zì¶• ê°ì‡ )
        for idx in self.pose_indices:
            lm = pose_landmarks[idx]
            rel_x = (lm.x - stable_center_x) / normalization_scale
            rel_y = (lm.y - stable_center_y) / normalization_scale
            rel_z = ((lm.z - stable_center_z) / normalization_scale) * 0.7
            keypoints.append([rel_x, rel_y, rel_z])

        # ì™¼ì†
        left_wrist_pos = None
        if hasattr(results, 'left_hand_landmarks') and results.left_hand_landmarks:
            hand_landmarks = results.left_hand_landmarks.landmark
            wrist = hand_landmarks[0]
            wrist_rel_x = (wrist.x - stable_center_x) / normalization_scale
            wrist_rel_y = (wrist.y - stable_center_y) / normalization_scale
            wrist_rel_z = ((wrist.z - stable_center_z) / normalization_scale) * 0.7
            left_wrist_pos = [wrist_rel_x, wrist_rel_y, wrist_rel_z]
            keypoints.append(left_wrist_pos)

            for i in range(1, 21):
                finger_lm = hand_landmarks[i]
                finger_rel_x = (finger_lm.x - wrist.x) / normalization_scale
                finger_rel_y = (finger_lm.y - wrist.y) / normalization_scale
                finger_rel_z = ((finger_lm.z - wrist.z) / normalization_scale) * 0.6
                keypoints.append([finger_rel_x, finger_rel_y, finger_rel_z])
        else:
            keypoints.extend([[0.0, 0.0, 0.0]] * 21)

        # ì˜¤ë¥¸ì†
        right_wrist_pos = None
        if hasattr(results, 'right_hand_landmarks') and results.right_hand_landmarks:
            hand_landmarks = results.right_hand_landmarks.landmark
            wrist = hand_landmarks[0]
            wrist_rel_x = (wrist.x - stable_center_x) / normalization_scale
            wrist_rel_y = (wrist.y - stable_center_y) / normalization_scale
            wrist_rel_z = ((wrist.z - stable_center_z) / normalization_scale) * 0.7
            right_wrist_pos = [wrist_rel_x, wrist_rel_y, wrist_rel_z]
            keypoints.append(right_wrist_pos)

            for i in range(1, 21):
                finger_lm = hand_landmarks[i]
                finger_rel_x = (finger_lm.x - wrist.x) / normalization_scale
                finger_rel_y = (finger_lm.y - wrist.y) / normalization_scale
                finger_rel_z = ((finger_lm.z - wrist.z) / normalization_scale) * 0.6
                keypoints.append([finger_rel_x, finger_rel_y, finger_rel_z])
        else:
            keypoints.extend([[0.0, 0.0, 0.0]] * 21)

        # ì–‘ì† ì†ëª© 3D ìƒëŒ€ê±°ë¦¬ ì¶”ê°€
        if left_wrist_pos and right_wrist_pos:
            hand_distance = [
                right_wrist_pos[0] - left_wrist_pos[0],
                right_wrist_pos[1] - left_wrist_pos[1],
                right_wrist_pos[2] - left_wrist_pos[2]
            ]
            keypoints.append(hand_distance)
        else:
            keypoints.append([0.0, 0.0, 0.0])

        return keypoints


class PositionalEncoding(nn.Module):
    """Sinusoidal Positional Encoding (batch_first)"""
    def __init__(self, d_model, max_len=500):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]


class CNN_BiLSTM_Attention(nn.Module):
    """CNN-BiLSTM-Attention ëª¨ë¸"""
    def __init__(self, input_size=147, num_classes=7, cnn_channels=None, lstm_hidden=128, dropout=0.5):
        if cnn_channels is None:
            cnn_channels = [64, 128, 256]
        super().__init__()
        self.conv_layers = nn.ModuleList()
        in_channels = input_size
        for out_channels in cnn_channels:
            self.conv_layers.append(nn.Sequential(
                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm1d(out_channels),
                nn.ReLU(),
                nn.Dropout(dropout * 0.5)
            ))
            in_channels = out_channels
        self.lstm = nn.LSTM(
            input_size=cnn_channels[-1],
            hidden_size=lstm_hidden,
            num_layers=2,
            batch_first=True,
            bidirectional=True,
            dropout=dropout
        )
        self.attention = nn.MultiheadAttention(
            embed_dim=lstm_hidden * 2,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        self.classifier = nn.Sequential(
            nn.Linear(lstm_hidden * 2, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_classes)
        )
        self.layer_norm = nn.LayerNorm(lstm_hidden * 2)
        self.temporal_weight = nn.Parameter(torch.tensor(0.1))
        self.change_detector = nn.Sequential(
            nn.Linear(lstm_hidden * 2, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        self.pos_encoding = PositionalEncoding(d_model=lstm_hidden * 2, max_len=500)

    def forward(self, x):
        batch_size, seq_len, features = x.size()
        x_cnn = x.transpose(1, 2)
        for conv_layer in self.conv_layers:
            x_cnn = conv_layer(x_cnn)
        x_cnn = x_cnn.transpose(1, 2)
        lstm_out, _ = self.lstm(x_cnn)
        lstm_out = self.layer_norm(lstm_out)
        lstm_out = self.pos_encoding(lstm_out)
        change_scores = self.change_detector(lstm_out).squeeze(-1)
        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)
        change_weights = change_scores.unsqueeze(-1) * self.temporal_weight
        weighted_attn = attn_out * (1 + change_weights)
        context_vector = weighted_attn.mean(dim=1)
        output = self.classifier(context_vector)
        return output, attn_weights.mean(dim=1), change_scores


class RealtimeSignPredictor:
    """ì‹¤ì‹œê°„ ìˆ˜í™” ì˜ˆì¸¡ê¸°"""

    def __init__(self, model_path: str, device: str = 'auto', max_frames: int = 300, buffer_size: int = 60):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device == 'auto' else torch.device(device)
        self.max_frames = max_frames
        self.buffer_size = buffer_size

        # ëª¨ë¸ ë¡œë”© - ì‚¬ìš©ì ì œì•ˆ ë°©ì‹ ì ìš©
        print(f"ğŸ”¥ ëª¨ë¸ ë¡œë”©: {model_path}")

        try:
            # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not Path(model_path).exists():
                raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

            # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (CPUë¡œ ê°•ì œ ë§¤í•‘í•˜ì—¬ ì¥ì¹˜ ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°)
            print("ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
            checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ")

            if not isinstance(checkpoint, dict):
                raise ValueError("ì²´í¬í¬ì¸íŠ¸ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")

            print(f"ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ í‚¤: {list(checkpoint.keys())}")

            # 3. ë ˆì´ë¸” ì¸ì½”ë” ë¨¼ì € ë¡œë“œ (í´ë˜ìŠ¤ ìˆ˜ í™•ì¸ì„ ìœ„í•´)
            if 'label_encoder' not in checkpoint:
                raise KeyError("ì²´í¬í¬ì¸íŠ¸ì— 'label_encoder'ê°€ ì—†ìŠµë‹ˆë‹¤")

            self.label_encoder = checkpoint['label_encoder']
            num_classes = len(self.label_encoder.classes_)
            print(f"ğŸ·ï¸ ë ˆì´ë¸” ì¸ì½”ë” ë¡œë“œ ì„±ê³µ: {list(self.label_encoder.classes_)}")
            print(f"ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {num_classes}")

            # 4. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì •í™•í•œ íŒŒë¼ë¯¸í„°ë¡œ)
            print("ğŸ—ï¸ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
            self.model = CNN_BiLSTM_Attention(
                input_size=147,  # 49 landmarks * 3 coordinates = 147
                num_classes=num_classes,
                cnn_channels=[64, 128, 256],
                lstm_hidden=128,
                dropout=0.5
            )
            print("âœ… ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")

            # 5. state_dict ë¡œë“œ
            if 'model_state_dict' in checkpoint:
                print("ğŸ”§ model_state_dict ë¡œë”© ì¤‘...")
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print("âœ… model_state_dict ë¡œë“œ ì„±ê³µ")
            elif 'model' in checkpoint:
                print("ğŸ”§ ì €ì¥ëœ ëª¨ë¸ì—ì„œ state_dict ì¶”ì¶œ ì¤‘...")
                # ì €ì¥ëœ ëª¨ë¸ì´ state_dictì¸ì§€ í™•ì¸
                if isinstance(checkpoint['model'], dict):
                    self.model.load_state_dict(checkpoint['model'])
                    print("âœ… state_dictë¡œ ë¡œë“œ ì„±ê³µ")
                else:
                    # ëª¨ë¸ ê°ì²´ì—ì„œ state_dict ì¶”ì¶œ ì‹œë„
                    try:
                        if hasattr(checkpoint['model'], 'state_dict'):
                            self.model.load_state_dict(checkpoint['model'].state_dict())
                            print("âœ… ëª¨ë¸ ê°ì²´ì—ì„œ state_dict ì¶”ì¶œ ì„±ê³µ")
                        else:
                            raise AttributeError("ëª¨ë¸ ê°ì²´ì— state_dict ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                    except Exception as e:
                        print(f"âš ï¸ ëª¨ë¸ ê°ì²´ì—ì„œ state_dict ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        print("ğŸ†• ìƒˆ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤")
            else:
                print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ì— ëª¨ë¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                print("ğŸ†• ìƒˆ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤")

            # 6. ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•˜ê³  ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model.eval()
            self.model = self.model.to(self.device)

            # 7. ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

            print(f"âœ… ëª¨ë¸ ì„¤ì • ì™„ë£Œ!")
            print(f"  - Device: {self.device}")
            print(f"  - Classes: {list(self.label_encoder.classes_)}")
            print(f"  - Total parameters: {total_params:,}")
            print(f"  - Trainable parameters: {trainable_params:,}")

        except FileNotFoundError as e:
            print(f"âŒ íŒŒì¼ ì˜¤ë¥˜: {e}")
            self._initialize_default_model()
        except KeyError as e:
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ í‚¤ ì˜¤ë¥˜: {e}")
            self._initialize_default_model()
        except ValueError as e:
            print(f"âŒ ê°’ ì˜¤ë¥˜: {e}")
            self._initialize_default_model()
        except RuntimeError as e:
            print(f"âŒ PyTorch ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
            print("ğŸ’¡ PyTorch ë²„ì „ ë¶ˆì¼ì¹˜ ë˜ëŠ” ëª¨ë¸ êµ¬ì¡° ë¶ˆì¼ì¹˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            self._initialize_default_model()
        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            self._initialize_default_model()

        # 8. ì¢Œí‘œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        print("ğŸ¯ ì¢Œí‘œ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì¤‘...")
        try:
            self.extractor = CoordinateExtractor()
            print("âœ… ì¢Œí‘œ ì¶”ì¶œê¸° ì¤€ë¹„ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì¢Œí‘œ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

        # 9. í”„ë ˆì„ ë²„í¼ ì´ˆê¸°í™”
        self.frame_buffer = []
        self.current_position = 0
        print(f"ğŸ“¦ í”„ë ˆì„ ë²„í¼ ì¤€ë¹„ ì™„ë£Œ (í¬ê¸°: {buffer_size})")

        print("ğŸš€ ì‹¤ì‹œê°„ ìˆ˜í™” ì˜ˆì¸¡ê¸° ì´ˆê¸°í™” ì™„ë£Œ!")

    def _initialize_default_model(self):
        """ê¸°ë³¸ ëª¨ë¸ë¡œ ì´ˆê¸°í™”"""
        print("ğŸ†• ê¸°ë³¸ ëª¨ë¸ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
        try:
            from sklearn.preprocessing import LabelEncoder

            # ê¸°ë³¸ ë ˆì´ë¸” ì¸ì½”ë” ìƒì„±
            self.label_encoder = LabelEncoder()
            # ì¼ë°˜ì ì¸ í•œêµ­ ìˆ˜í™” ë‹¨ì–´ë“¤ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
            default_classes = ['ê°€ì¡±', 'ê³ ë§™ë‹¤', 'ë‚˜', 'ì‚¬ë‘', 'ì•ˆë…•', 'ì•„ë‹ˆë‹¤', 'ì¢‹ë‹¤']
            self.label_encoder.classes_ = np.array(default_classes)

            # ê¸°ë³¸ ëª¨ë¸ ìƒì„±
            self.model = CNN_BiLSTM_Attention(
                input_size=147,
                num_classes=len(default_classes),
                cnn_channels=[64, 128, 256],
                lstm_hidden=128,
                dropout=0.5
            )

            self.model.eval()
            self.model = self.model.to(self.device)

            print(f"âœ… ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"  - Device: {self.device}")
            print(f"  - Default classes: {default_classes}")
            print("âš ï¸ ì£¼ì˜: í•™ìŠµëœ ê°€ì¤‘ì¹˜ê°€ ì•„ë‹Œ ëœë¤ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")

        except Exception as e:
            print(f"âŒ ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e}")
            raise RuntimeError("ëª¨ë¸ ì´ˆê¸°í™”ì— ì™„ì „íˆ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

    def add_frame(self, frame: np.ndarray) -> Optional[dict]:
        """í”„ë ˆì„ì„ ë²„í¼ì— ì¶”ê°€í•˜ê³  ì˜ˆì¸¡ ìˆ˜í–‰"""
        # ì¢Œí‘œ ì¶”ì¶œ
        coords = self.extractor.extract_from_frame(frame)
        if not coords:
            return None

        # ë²„í¼ì— ì¶”ê°€
        if len(self.frame_buffer) < self.buffer_size:
            self.frame_buffer.append(coords)
        else:
            # ì›í˜• ë²„í¼: ì˜¤ë˜ëœ í”„ë ˆì„ êµì²´
            self.frame_buffer[self.current_position] = coords
            self.current_position = (self.current_position + 1) % self.buffer_size

        # ì¶©ë¶„í•œ í”„ë ˆì„ì´ ìŒ“ì˜€ì„ ë•Œë§Œ ì˜ˆì¸¡
        if len(self.frame_buffer) >= 30:  # ìµœì†Œ 30í”„ë ˆì„
            return self._predict_from_buffer()

        return {
            'status': 'collecting',
            'frames_collected': len(self.frame_buffer),
            'message': f'í”„ë ˆì„ ìˆ˜ì§‘ ì¤‘... ({len(self.frame_buffer)}/{self.buffer_size})'
        }

    def _predict_from_buffer(self) -> dict:
        """ë²„í¼ì˜ í”„ë ˆì„ë“¤ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰"""
        try:
            # í˜„ì¬ ë²„í¼ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            keypoints = np.array(self.frame_buffer)  # (frames, 49, 3)

            # Zì¶• í‰í™œí™” (í•™ìŠµ ì‹œì™€ ë™ì¼)
            for landmark_idx in range(keypoints.shape[1]):
                z_seq = keypoints[:, landmark_idx, 2]
                smoothed_z = self._smooth_z_coordinates(z_seq)
                keypoints[:, landmark_idx, 2] = smoothed_z

            # ì „ì²˜ë¦¬
            input_tensor = self._preprocess_keypoints(keypoints)

            # ì˜ˆì¸¡
            with torch.no_grad():
                outputs, _, _ = self.model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                predicted_class = torch.argmax(probabilities, dim=1).cpu().numpy()[0]
                confidence = probabilities[0, predicted_class].cpu().numpy()
                predicted_label = self.label_encoder.inverse_transform([predicted_class])[0]

                # ëª¨ë“  í´ë˜ìŠ¤ í™•ë¥ 
                all_probs = probabilities.cpu().numpy()[0]
                class_probabilities = {}
                for i, class_name in enumerate(self.label_encoder.classes_):
                    class_probabilities[class_name] = float(all_probs[i])

            return {
                'status': 'predicted',
                'predicted_label': predicted_label,
                'confidence': float(confidence),
                'class_probabilities': class_probabilities,
                'frames_used': len(self.frame_buffer)
            }

        except Exception as e:
            return {
                'status': 'error',
                'error': str(e)
            }

    def _smooth_z_coordinates(self, z_sequence):
        """Zì¶• ì¢Œí‘œ ì´ë™í‰ê· (3í”„ë ˆì„) í•„í„°"""
        if len(z_sequence) < 3:
            return z_sequence
        smoothed = []
        for i in range(len(z_sequence)):
            if i == 0 or i == len(z_sequence) - 1:
                smoothed.append(z_sequence[i])
            else:
                avg = (z_sequence[i-1] + z_sequence[i] + z_sequence[i+1]) / 3
                smoothed.append(avg)
        return smoothed

    def _preprocess_keypoints(self, keypoints: np.ndarray) -> torch.Tensor:
        """ì¢Œí‘œ ì „ì²˜ë¦¬ (í•™ìŠµ ì‹œì™€ ë™ì¼)"""
        # (frames, 49, 3) â†’ (frames, 147)
        keypoints = keypoints.reshape(keypoints.shape[0], -1)

        # íŒ¨ë”©/ìë¥´ê¸°
        if keypoints.shape[0] < self.max_frames:
            pad_size = self.max_frames - keypoints.shape[0]
            keypoints = np.vstack([
                keypoints,
                np.zeros((pad_size, keypoints.shape[1]))
            ])
        else:
            keypoints = keypoints[:self.max_frames]

        return torch.FloatTensor(keypoints).unsqueeze(0).to(self.device)

    def reset_buffer(self):
        """ë²„í¼ ì´ˆê¸°í™”"""
        self.frame_buffer.clear()
        self.current_position = 0


# ì „ì—­ ì˜ˆì¸¡ê¸° ì¸ìŠ¤í„´ìŠ¤
predictor_instance = None

def get_predictor() -> RealtimeSignPredictor:
    """ì˜ˆì¸¡ê¸° ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤)"""
    global predictor_instance
    if predictor_instance is None:
        model_path = Path(__file__).parent / "cnn_bilstm_attention_model.pth"
        predictor_instance = RealtimeSignPredictor(str(model_path))
    return predictor_instance


class SignLanguagePredictor:
    def __init__(self, model_path='models/cnn_bilstm_attention_model.pth'):
        self.device = torch.device('cpu')  # CPU ì‚¬ìš© (ì„œë²„ í™˜ê²½ ê³ ë ¤)
        self.model = None
        self.label_encoder = None
        self.load_model(model_path)

    def load_model(self, model_path):
        try:
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = torch.load(model_path, map_location='cpu')

            # ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ
            self.label_encoder = checkpoint['label_encoder']

            # ëª¨ë¸ ìƒì„±
            self.model = CNN_BiLSTM_Attention(
                input_size=147,
                num_classes=len(self.label_encoder.classes_),
                cnn_channels=[64, 128, 256],
                lstm_hidden=128,
                dropout=0.5
            )

            # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()

            print(f"ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. í´ë˜ìŠ¤ ìˆ˜: {len(self.label_encoder.classes_)}")
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ í´ë˜ìŠ¤: {list(self.label_encoder.classes_)}")

        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e

    def predict(self, keypoints_data):
        """
        í‚¤í¬ì¸íŠ¸ ë°ì´í„°ë¡œë¶€í„° ìˆ˜ì–´ ì˜ˆì¸¡

        Args:
            keypoints_data: numpy array ë˜ëŠ” list, shape should be (sequence_length, 147)

        Returns:
            dict: ì˜ˆì¸¡ ê²°ê³¼ ë° í™•ë¥ 
        """
        try:
            if self.model is None:
                return {"error": "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

            # ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
            if isinstance(keypoints_data, list):
                keypoints_data = np.array(keypoints_data)

            # ì°¨ì› í™•ì¸ ë° ì¡°ì •
            if len(keypoints_data.shape) == 2:
                keypoints_data = keypoints_data.reshape(1, *keypoints_data.shape)

            # í…ì„œë¡œ ë³€í™˜
            input_tensor = torch.FloatTensor(keypoints_data).to(self.device)

            # ì˜ˆì¸¡
            with torch.no_grad():
                outputs = self.model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                predicted_class_idx = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_class_idx].item()

            # í´ë˜ìŠ¤ëª… ë³€í™˜
            predicted_class = self.label_encoder.inverse_transform([predicted_class_idx])[0]

            # ìƒìœ„ 3ê°œ ì˜ˆì¸¡ ê²°ê³¼
            top3_probs, top3_indices = torch.topk(probabilities[0], min(3, len(self.label_encoder.classes_)))
            top3_predictions = []

            for i, (prob, idx) in enumerate(zip(top3_probs, top3_indices)):
                class_name = self.label_encoder.inverse_transform([idx.item()])[0]
                top3_predictions.append({
                    "class": class_name,
                    "confidence": prob.item()
                })

            return {
                "predicted_class": predicted_class,
                "confidence": confidence,
                "top3_predictions": top3_predictions,
                "success": True
            }

        except Exception as e:
            return {
                "error": f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "success": False
            }

    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if self.model is None or self.label_encoder is None:
            return {"error": "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        return {
            "num_classes": len(self.label_encoder.classes_),
            "classes": list(self.label_encoder.classes_),
            "input_size": 147,
            "model_loaded": True
        }
